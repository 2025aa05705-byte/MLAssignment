# -*- coding: utf-8 -*-
"""train_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kDY-uXmtDtDwyOO8AwXtPOcpdQjOzOmj
"""

import pandas as pd
import numpy as np
import os
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef

# Import necessary libraries for handling zip file download
import requests
import zipfile
import io

# 1. Setup Directories
if not os.path.exists('model'):
    os.makedirs('model')

# 2. Load Dataset (UCI Statlog Vehicle Silhouettes)
# Columns based on dataset description (corrected 'MAX.LENGTH_ASPECT_ASPECT_RATIO')
cols = ['COMPACTNESS', 'CIRCULARITY', 'DISTANCE_CIRCULARITY', 'RADIUS_RATIO',
        'PR.AXIS_ASPECT_RATIO', 'MAX.LENGTH_ASPECT_RATIO', 'SCATTER_RATIO',
        'ELONGATEDNESS', 'PR.AXIS_RECTANGULARITY', 'MAX.LENGTH_RECTANGULARITY',
        'SCALED_VARIANCE_MAJOR', 'SCALED_VARIANCE_MINOR', 'SCALED_RADIUS_OF_GYRATION',
        'SKEWNESS_MAJOR', 'SKEWNESS_MINOR', 'KURTOSIS_MINOR', 'KURTOSIS_MAJOR',
        'HOLLOWS_RATIO', 'CLASS']

# Updated URL for the dataset (points to the zip file containing split data)
zip_url = "https://archive.ics.uci.edu/static/public/149/statlog+vehicle+silhouettes.zip"

# Download the zip file
response = requests.get(zip_url)
response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)

# Read the zip file content into a BytesIO object
zip_file_content = io.BytesIO(response.content)

# Extract and concatenate the multiple .dat files from the zip
df_list = []
with zipfile.ZipFile(zip_file_content, 'r') as z:
    file_list = z.namelist()
    print(f"Files in zip archive: {file_list}")
    for f_name in file_list:
        # The actual data files are xaa.dat, xab.dat, ..., xai.dat
        if f_name.startswith('x') and f_name.endswith('.dat'):
            print(f"Reading data from: {f_name}")
            with z.open(f_name) as f:
                # The dataset is space separated, using delim_whitespace for robust parsing
                df_part = pd.read_csv(f, sep='\s+', names=cols)
                df_list.append(df_part)

# Concatenate all parts into a single DataFrame
df = pd.concat(df_list, ignore_index=True)
df.dropna(inplace=True)

print(f"Dataset Loaded. Shape: {df.shape}")

# 3. Preprocessing
X = df.drop('CLASS', axis=1)
y = df['CLASS']

# Save a sample CSV for the user to upload in Streamlit
sample_test = df.sample(50)
sample_test.to_csv("sample_test_data.csv", index=False)
print("sample_test_data.csv created (Use this to upload in the Streamlit App)")

# Encode Target (Vehicle classes are strings: opel, saab, bus, van)
le = LabelEncoder()
y_encoded = le.fit_transform(y)
joblib.dump(le, 'model/label_encoder.pkl')

# Scale Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
joblib.dump(scaler, 'model/scaler.pkl')

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)

# 4. Define Models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, multi_class='ovr'),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Naive Bayes": GaussianNB(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(eval_metric='mlogloss', random_state=42) # Removed use_label_encoder
}

# 5. Train, Evaluate and Save
results = []

print("\n--- Training Models & Calculating Metrics ---")

for name, model in models.items():
    # Train
    model.fit(X_train, y_train)

    # Save Model
    filename = f"model/{name.replace(' ', '_').lower()}.pkl"
    joblib.dump(model, filename)

    # Predict
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    # AUC for multiclass requires 'ovr'
    auc = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')
    prec = precision_score(y_test, y_pred, average='macro')
    rec = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')
    mcc = matthews_corrcoef(y_test, y_pred)

    results.append({
        "Model": name,
        "Accuracy": round(acc, 4),
        "AUC": round(auc, 4),
        "Precision": round(prec, 4),
        "Recall": round(rec, 4),
        "F1 Score": round(f1, 4),
        "MCC": round(mcc, 4)
    })

# 6. Output Table for README
results_df = pd.DataFrame(results)
print("\nCOPY THIS TABLE FOR YOUR README.MD and PDF:")
print(results_df.to_markdown(index=False))

# Create dummy observation text for user convenience
print("\n--- Model Observations (Copy to README) ---")
best_model = results_df.loc[results_df['Accuracy'].idxmax()]
print(f"1. {best_model['Model']} performed the best with an accuracy of {best_model['Accuracy']}.")
print("2. XGBoost and Random Forest generally outperform linear models due to the non-linear nature of image silhouette features.")
print("3. Naive Bayes assumes independence between features, which might not hold true for geometric measurements, potentially leading to lower scores.")